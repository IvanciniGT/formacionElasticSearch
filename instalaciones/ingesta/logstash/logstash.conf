input {
  beats {
    port => 5044
  }
}

# Aqui le vamos a meter el procesamiento de cada documento
filter {
  grok {
    match => {
      #"message" => "(?<ip>[0-9.]+)"        (?<CAMPO>PATRON)
      #             %{PATRON_PREESTABLECIDO:campo}
      "message" => "%{HTTPD_COMBINEDLOG}"                    
    }
  }
  mutate {
    remove_field => ["agent", "@timestamp", "@metadata","[log][file]", "ecs", "event"]
    lowercase    => ["[http][request][method]"]
  }
  date {
    match        => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
    remove_field => [ "timestamp" ]
  }
  geoip {
    source       => [ "[source][address]" ]
    target       => "posicionamiento"
    remove_field => [ "source" ]
  }
}


output {
  # En producción esto va fuera !
  stdout { 
    codec => rubydebug #json 
  }
  elasticsearch {
    hosts       => ["https://172.31.41.172:8080"]
    user        => elastic
    password    => password
    ssl_enabled => true
    # antiguamente era cacert
    ssl_certificate_authorities => ["/usr/share/logstash/certs/ca/ca.crt"]
    ssl_verification_mode       => "full"
    index           => "apache-%{+YYYY.MM}"
    # Logstash es el que va a crear esa plantilla y como el nombre del índice que estamos usando encaja con el patrón que hemos definido en la plantilla,
    # Elastic aplicará los mappings y los settings a ese índice...
    # Indice que si no existe, será creado en automático
    manage_template => true
    template_name   => "plantilla-apache-"
    template        => "/usr/share/logstash/templates/plantilla.json"
  }
}
